import sys
!pip install -U langchain langchain-core langchain-google-genai scikit-learn pandas
import pandas as pd
from sklearn.linear_model import LogisticRegression

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
data = {
    "age": [22, 35, 45, 60],
    "bmi": [19, 28, 32, 30],
    "smoking": [0, 1, 1, 0],
    "risk": [0, 1, 1, 1]
}

df = pd.DataFrame(data)
df
X = df[["age", "bmi", "smoking"]]
y = df["risk"]

ml_model = LogisticRegression()
ml_model.fit(X, y)

print("ML model trained")
age = int(input("Enter age: "))
bmi = float(input("Enter BMI: "))
smoking = int(input("Smoking? (1 = Yes, 0 = No): "))
prediction = ml_model.predict([[age, bmi, smoking]])

risk = "High Risk" if prediction[0] == 1 else "Low Risk"
print("Predicted Risk:", risk)
from google.colab import userdata

api_key = userdata.get("GEMINI_API_KEY")

if api_key is None:
    raise ValueError("GEMINI_API_KEY not found in Colab Secrets")

print("API key loaded securely")
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="models/gemini-2.5-flash",  # Use a model from list_models()
    google_api_key=api_key
)

print("Gemini LLM initialized with models/gemini-2.5-flash")
import google.generativeai as genai
genai.configure(api_key=api_key)

print("Available models for generateContent:")
for m in genai.list_models():
  if "generateContent" in m.supported_generation_methods:
    print(m.name)

health_prompt = PromptTemplate(
    input_variables=["age", "bmi", "smoking", "risk"],
    template="""
You are an AI health assistant.

User information:
Age: {age}
BMI: {bmi}
Smoking habit: {smoking}
Predicted health risk: {risk}

Explain the risk in simple words.
Give exactly 3 lifestyle improvement tips.
Add a disclaimer that this is not medical advice.
"""
)
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="models/gemini-2.5-flash",  # Use a model from list_models()
    google_api_key=api_key
)

health_chain = health_prompt | llm

print(f"LLM is configured to use model: {llm.model}")

response = health_chain.invoke({
    "age": age,
    "bmi": bmi,
    "smoking": smoking,
    "risk": risk
})

print(response.content)